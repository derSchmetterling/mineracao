---
title: "Modelos Alternativos"
author: "Pedro Vinicius Alves"
date: "2023-04-11"
output: pdf_document
---



```{r setup, include=FALSE}

#: warning: false
#| echo: false

library(randomForest)
library(dplyr)
library(R2jags)
library(coda)
```

## Leitura dos Dados


```{r cars}
data <- read.csv2('basetreino.csv', sep = ',')

data_test <- read.csv2('baseteste.csv', sep = ',')

data_test <- data_test %>%
  mutate_at(vars("URBAN", "MEN", "PRIVATE", "y"), as.factor)

data_reg <- data %>%
  mutate_at(vars("URBAN", "MEN", "PRIVATE", "y"), as.factor)

head(data)
```

### Reversal Power Links

Não precisa ser rodado.

```{r}
modelString="
model{
  for (i in 1:N) {
    y[i] ~ dbern(p[i])
    m[i] <- beta0+beta_urban*URBAN[i] + beta_sen*SEN[i] + beta_age*AGE[i] + beta_men*MEN[i]
    pstar[i] <- 1/3.141592* arctan(-m[i]) + 1/2
    p[i] <- 1-pow(pstar[i], lambda)
  
  }
  beta0 ~ dnorm(0,100)
  delta ~ dunif(-2,2)
  lambda <- exp(delta)
  beta_urban ~ dnorm(0,100)
  beta_sen ~ dnorm(0,100)
  beta_age ~ dnorm(0,100)
  beta_men ~ dnorm(0,100)
  
  }

"
writeLines(modelString, con='models/m1_cauchit.bug')
```




```{r}
N = nrow(data)
jagsData <- list(N = N, y = data$y,
    URBAN = data$URBAN, SEN = data$SENIORITY, 
    AGE = data$AGE, MEN = data$MEN)
```

```{r}
m1_blogit <- jags(data=jagsData,model.file='models/m1_cauchit.bug',
                   param=c('beta0','beta_urban', 'beta_sen', 'beta_age', 'beta_men', 'lambda'),
                   n.chains=3, n.iter=30000, n.burnin=5000, n.thin=10)
```


```{r}
mcmc.samples <- as.mcmc(m1_blogit)
save(mcmc.samples, file="models/mcmc_results.RData")
```


```{r}
library(lattice)

jagsfit.mcmc <- as.mcmc(m1_blogit)
densityplot(mcmc.samples)

HPDinterval(jagsfit.mcmc)
traceplot(jagsfit.mcmc)
```

## Predição no Conjunto de Teste

```{r}

predict_bcauchit <- function(jagsfit.mcmc, data) {
# data deve ser do tipo dataframe

#vetor da média dos coeficientes beta_age/beta_men/beta_sen/beta_urban
coef <- as.matrix(summary(jagsfit.mcmc)$statistics[, 'Mean'][1:5])
lambda <- as.matrix(summary(jagsfit.mcmc)$statistics[, 'Mean'])[7]


data_matrix <- data.matrix(data)[,c(-1,-4)][, c(3,1,4,2)]
data_matrix <- cbind(data_matrix, intercept = 1)
n <- dim(data_matrix)[1]


#coef * data
res <- t(coef) %*% t(data_matrix)
res_vec <- as.vector(res)


pstar <- 1/pi* atan(-res_vec) + 0.5
p <- 1- pstar^lambda

predictions <- rbinom(n, 1, p)

return(predictions)
}
```

```{r}
pred_train <- predict_bcauchit(jagsfit.mcmc, data)

```


```{r}
accuracy_bayes <- function(y, y_pred) {
  

  acc <- mean(y == y_pred)
  
  return(acc)
  
}

accuracy_bayes(data$y, pred_train)
```



## Posterior Predictive 

```{r}

# Generate posterior predictive samples
predictive.samples <- co(m1_blogit, newdata=data, nsim=1000)

plot(predictive.samples)

```


```{r}
samps <- coda.samples(m1_blogit, n.iter=10000 )

```



## RStan


```{r}
#| warning: false
library(rstan)
library(bayesplot)
```



```{r}
stanString <- "
data {
  int N;
  int<lower=0, upper=1> y[N];
  int<lower=0, upper=1> URBAN[N];
  int<lower=0, upper=1> MEN[N];
  int SEN[N];
  int AGE[N];
}

parameters {
  real beta0;
  real beta_urban;
  real beta_sen;
  real beta_age;
  real beta_men;
  real<lower=-2, upper=2> delta;
}

transformed parameters {
  real m[N];
  real pstar[N];
  real p[N];
  
  real lambda;
  lambda <- exp(delta);
  
  for (i in 1:N) {
    m[i] = beta0 + beta_urban * URBAN[i] + beta_sen * SEN[i] + beta_age * AGE[i] + beta_men * MEN[i];
    pstar[i] = 1 /(pi()) * atan(-m[i]) + 0.5;
    p[i] = 1 - pow(pstar[i], lambda);
  }
}

model {
  beta0 ~ normal(0, 100);
  beta_urban ~ normal(0, 100);
  beta_sen ~ normal(0, 100);
  beta_age ~ normal(0, 100);
  beta_men ~ normal(0, 100);
  delta ~ uniform(-2, 2);
  y ~ bernoulli(p);
}
"

writeLines(stanString, con = "models/cauchito.stan")
```




```{r}
N <- nrow(data)

data_list <- list(N = N, y = data$y, URBAN = data$URBAN, MEN = data$MEN, SEN = data$SEN, AGE = data$AGE)



# Compile model
model <- stan_model(model_code = stanString)

# Run MCMC chains
fit <- sampling(model, data = data_list, chains = 1, iter = 2000, warmup = 1000, seed = 123)

# Print summary of results
#print(fit)
```

```{r}

sampling(fit)
color_scheme_set("red")
ppc_dens_overlay(y = fit$y,
                 yrep = posterior_predict(fit, draws = 50))
```



### Erros ao rodar

Verifique se a versão do R for superior a 4.2 faça -> https://github.com/stan-dev/rstan/wiki/Configuring-C---Toolchain-for-Windows

```{r}

#install.packages("StanHeaders", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
#install.packages("rstan", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
```





### Acurácia Treino

Primeiramento tunamos o modelo para encontrar o melhor valor de mtry - número de variáveis amostradas aleatoriamente.
```{r}
t <- tuneRF(data_reg[,-1], data_reg[,1],
       stepFactor = 0.5,
       plot = TRUE,
       ntreeTry = 150,
       trace = TRUE,
       improve = 0.05)
```



```{r pressure, echo=FALSE}
rf <- randomForest(y~., data=data_reg, proximity=TRUE, mtry = 2) 
print(rf)
plot(rf)
```

```{r}
p1 <- predict(rf, data_reg)
cf_train <- caret::confusionMatrix(p1, data_reg$y)
cf_train
```

### Acurácia Teste

```{r}
p2 <- predict(rf, data_test)
cf_test <- caret::confusionMatrix(p2, data_test$y)
cf_test
```

```{r}
hist(treesize(rf),
     main = "No. of Nodes for the Trees",
     col = "green")
varImpPlot(rf,
           sort = T,
           n.var = 10,
           main = "Top 10 - Variable Importance")
importance(rf)
```

```{r}
partialPlot(rf, data_reg, SENIORITY, "1")
partialPlot(rf, data_reg, URBAN, "1")
partialPlot(rf, data_reg, MEN, "1")
partialPlot(rf, data_reg, PRIVATE, "1")
partialPlot(rf, data_reg, AGE, "1")

```

```{r}
MDSplot(rf, data_reg$y)
```






```{r}

data_reg$y <- as.integer(data_reg$y)

data_reg %>% {
  cat(sprintf(
    "Número de indivíduos com cobertura completa de seguro: %s (%.2f%% of total)\n",
    nrow(filter(data_reg,y == 1)), 100*nrow(filter(data_reg,y == 1))/nrow(data_reg)))
}
```





