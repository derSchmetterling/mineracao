---
title: "Desafio 2"
subtitle: "Predizendo probabilidade de adquirir aluguel para máquinas"
author: Pedro Vinícius Alves Silva - 10727865
format:
  html:
    embed-resources: true
    fontsize: 15pt
    theme: sandstone
    code-fold: true
    echo: true
    number-sections: false
    code-tools: true
    toc: true
    df-print: paged
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Bibliotecas

```{r bibliotecas, warning=FALSE, message=FALSE}

library(lares)
library(ggplot2)
library(dplyr)
library(readxl)
library(GGally)
library(gamlss)
library(nnet)
library(caret)
```

# Amostragem e divisão treino e teste

Procedimentos de amostragem e divisão de treino e teste estão disponíveis no arquivo .qmd.

```{r}
# data <-  read_excel('ChoiceBehaviour.xlsx')
# summary(data)
```

```{r}
#| echo: false
# set.seed(10727865)
# sampled_df <- sample_n(data, 2000)
# write.csv(sampled_df,'baseprincipal.csv', sep = '/t', row.names = F)
# 
# sampled_df <- data.frame(sampled_df)
# 
# sampled_df[c(1,2),]
```

```{r}
#| echo: false
# train_idx <- caret::createDataPartition(sampled_df$Resposta, p =0.7, list = FALSE)
# 
# train_df <- sampled_df[train_idx,]
# test_df <- sampled_df[-train_idx,]
# 
# write.csv(train_df,'basetreino.csv', sep = '/t', row.names = F)
# 
# write.csv(test_df,'baseteste.csv', sep = '/t', row.names = F)
```

# Análise Exploratória

## Descrição do problema

Temos um conjunto de dados referentes ao aluguel de três máquinas por agricultores, as colunas são referentes a:

-   **Regiao:** variável discreta representando local onde moram os agricultores

-   **Prima:** variável discreta representando valor que o agricultor precisa pagar para reparar a máquina

-   **CustoProduto:** variável inteira positiva representando custo estimado do maquinário no momento do aluguel pelo agricultor

-   **Pmaquinabase:** variável contínua positiva representando o preço do aluguel da máquina base para dar cobertura ao assegurado

-   **Pmaquina1:** variável contínua positiva representando o preço do aluguel da máquina 1 ofertado ao agricultor

-   **Pmaquina2:** variável contínua positiva representando o preço do aluguel da máquina 2 ofertado ao agricultor

-   **Pmaquina3:** variável contínua positiva representando o preço do aluguel da máquina 3 ofertado ao agricultor

-   **Cmaquina1:** variável contínua positiva representando o custo do aluguel da máquina 1 ofertado ao agricultor

-   **Cmaquina2:** variável contínua positiva representando o custo do aluguel da máquina 2 ofertado ao agricultor

-   **Cmaquina3:** variável contínua positiva representando o custo do aluguel da máquina 3 ofertado ao agricultor

-   **Resposta:** variável discreta, assume o valor de 0 quando o agricultor não aluga nenhum máquina ou o número relativa a qual máquina ele alugou (1,2, ou 3).

O agricultor conhece apenas o valor de reparo das máquina (Prima) e o preço de aluguel de cada uma (Pmaquina1,Pmaquina2,Pmaquina3), enquanto a companhia tem conhecimento de todas as variáveis. **Assim, queremos construir um modelo que prediza qual máquina determinado agricultor vai alugar, desconsiderando aqueles clientes que não alugaram nenhuma.**

```{r}
complete_data <- read.csv('baseprincipal.csv')
data_train <-  read.csv('basetreino.csv')
data_test <-  read.csv('baseteste.csv')

complete_data <-  complete_data %>% 
        mutate_at(vars(CustoProduto), as.integer) %>% 
        mutate_at(vars(Resposta), as.factor) %>% 
        filter(Resposta != '0')
  

data_train <- data_train %>% 
        mutate_at(vars(CustoProduto), as.integer) %>% 
        mutate_at(vars(Resposta), as.factor) %>% 
        filter(Resposta != '0')


data_test <- data_test %>% 
        mutate_at(vars(CustoProduto), as.integer) %>% 
        mutate_at(vars(Resposta), as.factor) %>% 
        filter(Resposta != '0')


head(data_train)
data_train
```

Inicialmente vemos que não há valores faltantes nem no conjunto de treino nem no conjunto de teste

```{r, message=FALSE, warning=FALSE}
library(VIM)
aggr(data_test, col = c('green','red'), numbers = TRUE, sortVars = TRUE, 
     labels = names(data_train), cex.axis = .5, gap = 2, 
     ylab = c("Proportion in variable","Proportion in dataset"))
```

```{r}
summary(data_train[, c('Pmaquina1', 'Pmaquina2', 'Pmaquina3' )])
summary(data_train[, c('Cmaquina1', 'Cmaquina2', 'Cmaquina3' )])
```

```{r}
hist(data_train[, c('Cmaquina1' )])
hist(data_train[, c('Cmaquina2' )])
hist(data_train[, c('Cmaquina3' )])
```

## Seleção de Features

### Correlações

Analisando a correlação linear entre as variáveis numéricas, notamos que as variáveis relativas ao custo das máquinas e o preço do aluguel estão fortemente relacionadas, como é de se esperar. Porém, é importante ressaltar como o custo de uma máquina está intimamente correlacionado com o preço de aluguel de outra ([*Pmaquina3*]{.underline} e *Cmaquina1* tem 0.82, por exemplo).

As variáveis *CustoProduto* e *Prima* foram as que apresentaram menor correlação com as outras, com coeficientes de menores ou iguais a 0.6, porém todas as correlações são estatisticamente diferentes de 0 ao nível de 5% de significância.

```{r}
#numeric <- c("AGE", "SENIORITY")
df_pearson<- data_train[, -c(1,2,12)]


correl<- cor(df_pearson, method = 'pearson')

testRes = corrplot::cor.mtest(df_pearson, conf.level = 0.95, method = 'pearson')

corrplot::corrplot(correl, p.mat = testRes$p, addCoef.col ='black',method = 'square', order = 'FPC', type = 'lower', insig = 'blank', number.cex=0.6)
```

Usando o coeficiente de Kendall e adicionando as covariáveis categóricas, nota-se como a *Regiao* tem correlação estatisticamente igual a zero com todas as outras covariáveis. É interessante ver também que *Cmaquina3 e Pmaquina1* não apresentam correlação com a variável Resposta e, de modo, geral, todas as covariáveis tem correlação monotônica fraca com a variável dependente.

```{r, warning=FALSE}

df_spearman <- data_train %>% 
  mutate_at(vars(Resposta), as.integer)

correl<- cor(df_spearman, method = 'kendall')

testRes = corrplot::cor.mtest(df_spearman, conf.level = 0.95, method = 'spearman')

corrplot::corrplot(correl, p.mat = testRes$p, addCoef.col ='black',method = 'square', order = 'FPC', type = 'lower', insig = 'blank', number.cex=0.6)
```

### ANOVA F-Score

Em seguida, utilizamos F-Score para escolher um conjunto de três variáveis para representar o conjunto de dados completo.

```{python}
import pandas as pd
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif


df = pd.read_csv('basetreino.csv')
df = df[df['Resposta'] != 0]

df['CustoProduto'] = df['CustoProduto'].astype(int)
df = df.drop('Id', axis = 1)
df['CustoProduto'] = df['CustoProduto'].astype(str).apply(lambda x: x[:-2])


```

```{python}

x = df.iloc[:,1:10]
y = df.iloc[:,10]


fs = SelectKBest(score_func=f_classif, k=3)
# aplica o método
X_selected = fs.fit_transform(x, y)
print('Features selecionadas: ')
fs.get_feature_names_out()
```

```{python}
col_names = ['Prima', 'Pmaquina2', 'Pmaquina3']
df_reduced = pd.DataFrame(X_selected, columns = col_names )
```

Vimos anteriormente também que *Pmaquina1,* Pmaquina*2*, *Pmaquina3* tem um correlação linear alta de acima de 0.9, então usaremos arbitrariamente apenas *Pmaquina1*, terminando com o seguinte conjunto de dados:

```{r}
# df_reg <- data_train %>% 
#   dplyr::select(c('Resposta', 'Pmaquina1', 'Prima'))
# 
# df_test <- data_test %>% 
#   dplyr::select(c('Resposta', 'Pmaquina1', 'Prima'))
# 
# 
# head(df_reg)

```

### Clusterização

Para termos uma ideia do perfil dos clientes, vamos aplicar um procedimento de agrupamento. Para isso, escalamos os dados númericos para que diferentes dimensões não afetem a análise de processo.

```{python}
#| warning: false

import pandas as pd
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from sklearn.preprocessing import StandardScaler

df_scaled = df.copy()
df_scaled.iloc[:,0:10] = StandardScaler().fit_transform(df_scaled.iloc[:,0:10])
df_scaled.head()
```

Aplicamos o agrupamento hierárquico usando o método 'ward'.

```{python}
from sklearn.cluster import AgglomerativeClustering

#complete


cluster = AgglomerativeClustering(n_clusters = 3,
distance_threshold=None,
compute_distances = True,
linkage='ward')
model = cluster.fit(df_scaled)

```

```{python}

df_cluster = df.copy()
df_cluster['HCluster'] = model.labels_.astype(str)
df_cluster.sort_values(by = 'HCluster', inplace=True)
df_cluster['CustoProduto'] = df_cluster['CustoProduto'].astype(int)


```

```{python}
import seaborn as sns
import matplotlib.pyplot as plt
sns.set_palette(sns.color_palette("tab10",4))
fig, ax = plt.subplots(1,2, figsize=(12,9))
fig.tight_layout(pad=6.0)

p1 = sns.scatterplot(data = df_cluster,y = 'Prima', x = 'Pmaquina1',
hue = 'HCluster',
ax = ax[0])
p1.set(title = 'Prima vs Pmaquina3',ylabel = 'Custo de Matéria Prima',xlabel = 'Preço da Máquina 1')

# p2 = sns.scatterplot(data = df_cluster,x = 'Prima', y = 'Pmaquina2',
# hue = 'HCluster',
# ax = ax[1])
# p2.set(title = 'Prima por Pmaquina3',xlabel = 'Custo de Matéria Prima',ylabel = 'Preço da Máquina 3')



p2 = sns.histplot(data = df_cluster,
x = 'Resposta',
hue = 'HCluster',
discrete = True,
multiple = 'stack',
ax = ax[1])
p2.set(title = 'Clusterização Hierárquica',
xlabel = 'Máquina Alugada',
ylabel = 'Frequência')



plt.show()
```

```{python}

from sklearn.cluster import KMeans
from sklearn.cluster import MiniBatchKMeans

cluster = KMeans(random_state = 10727865, n_clusters = 3, init = 'k-means++')
model_k = cluster.fit(df_scaled)


df_cluster['KMeans'] = model_k.labels_.astype(str)
df_cluster.sort_values(by = 'KMeans', inplace=True)
```

Ao plotar o custo de matéria prima e o preço da máquina 3 e analisando os agrupamentos gerados, conseguimos encontrar 3 grupos de clientes. Quando o preço da máquina 3 é alto, os clientes do grupo 1 parecem ter uma tendência maior a alugar a máquina 1. Analogamente, clientes do grupo 0, ao serem apresentados a um preço baixo da máquina 3, preferem alugar a máquina 2. Já os indivíduos do grupo 2, parecem ter uma preferência pelas máquinas 1 e 2, em detrimento da 3.

## Distribuição da Variável Resposta

Analisando a variável relativa ao aluguel de máquinas pelos agricultores, vemos que a grande maioria deles não alugou nenhuma.

```{r}
hist_resp <- ggplot(data_train %>% count(Resposta) %>%    
         mutate(pct=n/sum(n)),
       aes(as.factor(Resposta), n, fill = Resposta)) +
  geom_bar(stat="identity") +
  scale_fill_manual(values = c("#0072B2", "#D55E00", "#009E73"))+
  #geom_text(aes(label=paste0(sprintf("%1.1f", pct*100),"%")), position=position_stack(vjust=0.5)) +
  labs(x = 'Aluguel de Máquinas', y = 'Frequência Absoluta', title = "Distribuição de Resposta")

hist_resp
```

No total 144 indivíduos alugaram alguma das máquinas, sendo a máquina 2 a mais frequente.

```{r}
#n_diff_zero <- nrow(filter(data_train, Resposta != 0))
n_a1 <- nrow(filter(data_train, Resposta == 1))
n_a2 <- nrow(filter(data_train, Resposta == 2))
n_a3 <- nrow(filter(data_train, Resposta == 3))


train <- c(nrow(data_train),n_a1, n_a2, n_a3)



rent <-  data.frame(Alugadas = c(nrow(data_train),n_a1, n_a2, n_a3),
                    row.names = c('Total Alugadas','Alugou 1','Alugou 2', 'Alugou 3' ))

rent
```

### Resposta vs Custo

```{r}

hist_resp <- ggplot(data_train,
       aes(Prima, Pmaquina1, colour = as.factor(Resposta) )) +
  geom_point(stat="identity") +
  #geom_text(aes(label=paste0(sprintf("%1.1f", pct*100),"%")), position=position_stack(vjust=0.5)) +
  scale_colour_manual(guide_legend(title="Máquina Alugada"), values = c("#0072B2", "#D55E00", "#009E73")) + 
  #scale_x_discrete(guide = guide_axis(n.dodge=3))+
  labs(y = 'Preço da Máquina 1', x = 'Custo de Matéria Prima', title = "Distribuição de Resposta")

hist_resp

# hist_resp <- ggplot(data_train,
#        aes(Prima, Pmaquina1, colour = as.factor(Resposta) )) +
#   geom_point(stat="identity") +
#   #geom_text(aes(label=paste0(sprintf("%1.1f", pct*100),"%")), position=position_stack(vjust=0.5)) +
#   scale_fill_manual(guide_legend(title="Aluguel de Máquinas"), values = c("#D95F02", "#1B9E77", "blue")) + 
#   #scale_x_discrete(guide = guide_axis(n.dodge=3))+
#   labs(x = 'Aluguel de Máquinas', y = 'Frequência Absoluta', title = "Distribuição de Resposta")


hist_resp


```

```{r}
hist_resp <- ggplot(data_train,
       aes(CustoProduto, Prima, colour = as.factor(Resposta) )) +
  geom_point(stat="identity") +
  #geom_text(aes(label=paste0(sprintf("%1.1f", pct*100),"%")), position=position_stack(vjust=0.5)) +
  scale_colour_manual(guide_legend(title="Máquina Alugada"), values = c("#0072B2", "#D55E00", "#009E73")) + 
  #scale_x_discrete(guide = guide_axis(n.dodge=3))+
  labs(y = 'Preço da Máquina 1', x = 'Custo de Matéria Prima', title = "Distribuição de Resposta")

hist_resp
```

```{r}
knitr::knit_exit()
```

## 

## Ajuste de Modelos

### Modelo Multinomial

Usando as variáveis usadas para a construção das três componentes principais e excluindo aqueles com alta correlação linear entre si, terminamos com *Prima* e *Pmaquina3*

```{r}

df_reg <- data_train

df_reg$Resposta2 <- relevel(df_reg$Resposta, ref = "1")
complete_data$Resposta2 <- relevel(complete_data$Resposta, ref = "1")


m1 <- multinom(Resposta2 ~ Prima + CustoProduto + Regiao +  + Pmaquina1 + Pmaquina2 , data = df_reg)

summary(m1)

#unique(predict(m1, newdata = reduced_data1, type = 'class'))


```

```{r}
z <- summary(m1)$coefficients/summary(m1)$standard.errors
z

p <- (1 - pnorm(abs(z), 0, 1)) * 2
p
```

```{r}
preds_class <- predict(m1,type = 'class', newdata = data_test)
preds_score <- predict(m1,type = 'probs', newdata = data_test)

postResample(df_test$Resposta, preds_class)
```

```{r}


totalAccuracy <- c()
cv <- 10
cvDivider <- floor(nrow(complete_data) / (cv+1))

for (cv in seq(1:cv)) {
  # assign chunk to data test
  dataTestIndex <- c((cv * cvDivider):(cv * cvDivider + cvDivider))
  dataTest <- complete_data[dataTestIndex,]
  # everything else to train
  dataTrain <- complete_data[-dataTestIndex,]
 
  cylModel <-  multinom(Resposta2 ~ Prima + Pmaquina1 + Regiao + CustoProduto, data = dataTrain)
 
  pred <- predict(cylModel, newdata=dataTest, type="class")
 
  #  classification error
  cv_ac <- postResample(dataTest$Resposta2, pred)[[1]]
  print(paste('Current Accuracy:',cv_ac,'for CV:',cv))
  totalAccuracy <- c(totalAccuracy, cv_ac)
}
```

```{r}
 mean(totalAccuracy)  
```

### Random Forest

```{r}
library(randomForest)

df_reg[,1] <- droplevels(df_reg[,1])

t <- tuneRF(df_reg[,-c(1,4)], df_reg[,1],
       stepFactor = 0.5,
       plot = TRUE,
       ntreeTry = 150,
       trace = TRUE,
       improve = 0.05)

```

```{r}
rf <- randomForest(Resposta~ Prima + Pmaquina3, data=df_reg, proximity=TRUE, mtry = 2) 
print(rf)
```

## Bibliografia

https://amunategui.github.io/multinomial-neuralnetworks-walkthrough/index.html

https://stats.oarc.ucla.edu/r/dae/multinomial-logistic-regression/
